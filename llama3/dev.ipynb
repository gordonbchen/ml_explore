{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gordonbchen/code/ml_explore/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 1.24B\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> 128000\n",
      "<|eot_id|> 128009\n",
      "<|start_header_id|> 128006\n",
      "<|end_header_id|> 128007\n",
      "<|end_of_text|> 128001\n",
      "\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|> \n",
      "\n",
      "tensor([[128000, 128006,   9125, 128007,    198,   2675,    527,    264,  11190,\n",
      "          15592,  18328,    430,    374,    459,   6335,    304,  18682,  21579,\n",
      "            323,    264,  11944,    323,   2867,  11326,     13, 128009,    198,\n",
      "         128006,    882, 128007,    198,   6854,    499,  10552,    279,    445,\n",
      "          81101,  18112,     30, 128009,    198, 128006,  78191, 128007]],\n",
      "       device='cuda:0') \n",
      "\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Llama architecture is a type of transformer-based neural network designed for natural language processing (NLP) tasks, particularly for text classification, sentiment analysis, and question-answering. It's a popular architecture in the field of NLP, and I'd be happy to explain it in detail.\n",
      "\n",
      "**What is Llama?**\n",
      "\n",
      "LLaMA (Large Language Model Application) is a type of transformer model developed by Meta AI. It's an open-source, self-supervised learning model that uses a transformer architecture to process and understand natural language.\n",
      "\n",
      "**Components\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Llama architecture is a transformer-based language model developed by Meta AI. It's a variant of the transformer architecture, which is commonly used in natural language processing (NLP) tasks.\n",
      "\n",
      "Here's a high-level overview of the Llama architecture:\n",
      "\n",
      "**Components:**\n",
      "\n",
      "1. **Encoder:** The encoder is a self-attention mechanism that processes the input sequence and generates a representation of the input.\n",
      "2. **Decoder:** The decoder is also a self-attention mechanism that processes the output of the encoder and generates the final output.\n",
      "3. **Attention Mechanism\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "n_params = sum(p.numel() for p in hf_model.parameters())\n",
    "print(f\"Params: {n_params / (10**9):.2f}B\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "special_tokens = {\n",
    "    \"bot\": \"<|begin_of_text|>\",\n",
    "    \"eot\": \"<|eot_id|>\",\n",
    "    \"sh\": \"<|start_header_id|>\",\n",
    "    \"eh\": \"<|end_header_id|>\",\n",
    "    \"eos\": \"<|end_of_text|>\"\n",
    "}\n",
    "for token in special_tokens.values():\n",
    "    print(token, tokenizer.convert_tokens_to_ids(token))\n",
    "print()\n",
    "\n",
    "# https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "prompt = f\"\"\"{special_tokens['sh']}system{special_tokens['eh']}\n",
    "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.{special_tokens['eot']}\n",
    "{special_tokens['sh']}user{special_tokens['eh']}\n",
    "Can you explain the Llama architecture?{special_tokens['eot']}\n",
    "{special_tokens['sh']}assistant{special_tokens['eh']}\"\"\"\n",
    "print(prompt, \"\\n\")\n",
    "\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(tokens[\"input_ids\"], \"\\n\")\n",
    "\n",
    "generated = hf_model.generate(\n",
    "    **tokens,\n",
    "    max_length=len(tokens) + 156, \n",
    "    num_return_sequences=2,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(special_tokens[\"eot\"])\n",
    ")\n",
    "for generation in tokenizer.batch_decode(generated):\n",
    "    print(\"\\n\", \"-\"*80)\n",
    "    print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hf_model)\n",
    "print(hf_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig(vocab_size=128256, n_hidden_layers=16, d_model=2048, d_hidden=8192, rms_norm_eps=1e-05, max_pos=131072, rope_config=RopeConfig(base_freq=500000.0, scale_factor=32.0, high_freq_factor=4.0, low_freq_factor=1.0, og_max_pos=8192), d_head=64, n_heads=32, n_kv_heads=8)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RopeConfig:\n",
    "    base_freq: float = 500_000.0\n",
    "    \n",
    "    scale_factor: float = 32.0\n",
    "    high_freq_factor: float = 4.0\n",
    "    low_freq_factor: float = 1.0\n",
    "    og_max_pos: int = 8_192\n",
    "    \n",
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int = 128_256\n",
    "    n_hidden_layers: int = 16\n",
    "    d_model: int = 2_048\n",
    "    d_hidden: int = 8_192\n",
    "    rms_norm_eps: float = 1e-5\n",
    "\n",
    "    max_pos: int = 131_072\n",
    "    rope_config: RopeConfig = RopeConfig()\n",
    "\n",
    "    d_head: int = 64\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: int = 8\n",
    "\n",
    "llama_config = LlamaConfig()\n",
    "print(llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_proj.weight torch.Size([8192, 2048])\n",
      "up_proj.weight torch.Size([8192, 2048])\n",
      "down_proj.weight torch.Size([2048, 8192])\n",
      "SwiGLU max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU: https://arxiv.org/pdf/2002.05202v1\n",
    "    SiLU aka Swish: https://pytorch.org/docs/stable/generated/torch.nn.functional.silu.html\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(d_model, d_hidden, bias=False)\n",
    "        self.gate_proj = nn.Linear(d_model, d_hidden, bias=False)\n",
    "        self.down_proj = nn.Linear(d_hidden, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate = F.silu(self.gate_proj(x))\n",
    "        z = self.up_proj(x)\n",
    "        return self.down_proj(gate * z)\n",
    "\n",
    "hf_swiglu = hf_model.model.layers[0].mlp\n",
    "my_swiglu = SwiGLU(llama_config.d_model, llama_config.d_hidden).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "for name, param in hf_swiglu.state_dict().items():\n",
    "    print(name, param.shape)\n",
    "    my_swiglu.state_dict()[name].copy_(param)\n",
    "\n",
    "def check_diff(x: torch.Tensor, y: torch.Tensor, name: str = \"Tensors\", atol: float = 1e-5) -> None:\n",
    "    print(f\"{name} max diff: {(x - y).abs().max()}\")\n",
    "    assert torch.allclose(x, y, atol=atol), f\"{name} not allclose\"\n",
    "\n",
    "B, T, C = 8, 128, llama_config.d_model\n",
    "rand_x = torch.randn((B, T, C), device=\"cuda\", dtype=torch.bfloat16)\n",
    "check_diff(hf_swiglu(rand_x), my_swiglu(rand_x), name=\"SwiGLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 128, 64]) torch.Size([8, 32, 128, 64])\n",
      "Cos max diff: 0.0\n",
      "Sin max diff: 0.0\n",
      "RoPE Q max diff: 0.0\n",
      "RoPE K max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Encoding: https://arxiv.org/pdf/2104.09864\n",
    "    HuggingFace Positional Encoding explanation: https://huggingface.co/blog/designing-positional-encoding\n",
    "    HuggingFace Llama Positional Encoding: https://github.com/huggingface/transformers/blob/41b9b92b52215bed472c9a534a06abbc3a9a95cd/src/transformers/modeling_rope_utils.py#L322\n",
    "    \"\"\"\n",
    "    def __init__(self, llama_config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        rope_config = llama_config.rope_config\n",
    "        freqs = 1.0 / rope_config.base_freq ** (torch.arange(0, llama_config.d_head, 2) / llama_config.d_head) \n",
    "        wavelens = 2 * math.pi / freqs\n",
    "\n",
    "        low_freq_wavelen = rope_config.og_max_pos / rope_config.low_freq_factor\n",
    "        freqs[wavelens > low_freq_wavelen] /= rope_config.scale_factor\n",
    "\n",
    "        smooth_factor = ((rope_config.og_max_pos / wavelens) - rope_config.low_freq_factor) / (rope_config.high_freq_factor - rope_config.low_freq_factor)\n",
    "        smoothed_freqs = ((1.0 - smooth_factor) * freqs / rope_config.scale_factor) + (smooth_factor * freqs)\n",
    "        high_freq_wavelen = rope_config.og_max_pos / rope_config.high_freq_factor\n",
    "        is_medium_freq = (wavelens <= low_freq_wavelen) & (wavelens >= high_freq_wavelen)\n",
    "        freqs = torch.where(is_medium_freq, smoothed_freqs, freqs)\n",
    "\n",
    "        pos = torch.arange(llama_config.max_pos).unsqueeze(1)\n",
    "        pthetas = pos * freqs  # (max_pos, d_head / 2)\n",
    "        self.register_buffer(\"sin\", pthetas.sin().repeat(1, 2))\n",
    "        self.register_buffer(\"cos\", pthetas.cos().repeat(1, 2))\n",
    "\n",
    "    def rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = x[..., :x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        T = x.shape[-2]\n",
    "        return (self.cos[None, None, :T] * x) + (self.sin[None, None, :T] * self.rotate_half(x))\n",
    "\n",
    "Q = torch.rand(B, llama_config.n_heads, T, llama_config.d_head, device=\"cuda\", dtype=torch.bfloat16)\n",
    "K = torch.rand_like(Q)\n",
    "affin_no_rope = Q @ K.transpose(-1, -2)\n",
    "\n",
    "rope = RoPE(llama_config).to(\"cuda\", dtype=torch.bfloat16)\n",
    "rope_q, rope_k = rope(Q), rope(K)\n",
    "print(rope_q.shape, rope_k.shape)\n",
    "\n",
    "# RoPE is a relative positional encoding, affinities on the diagonal should be the same.\n",
    "# This check does not work well for lower precision (bfloat16).\n",
    "# affin_rope = rope_q @ rope_k.transpose(-1, -2)\n",
    "# affin_rope_diags = torch.diagonal(affin_rope, dim1=-2, dim2=-1)\n",
    "# affin_no_rope_diags = torch.diagonal(affin_no_rope, dim1=-2, dim2=-1)\n",
    "# check_diff(affin_rope_diags, affin_no_rope_diags, name=\"RoPE diags\")\n",
    "\n",
    "hf_rope = hf_model.model.rotary_emb\n",
    "position_ids = torch.arange(T).expand(B, T).to(\"cuda\")\n",
    "cos, sin = hf_rope(Q, position_ids)\n",
    "check_diff(cos, rope.cos[:T], name=\"Cos\")\n",
    "check_diff(sin, rope.sin[:T], name=\"Sin\")\n",
    "hf_rope_q, hf_rope_k = apply_rotary_pos_emb(Q, K, cos, sin)\n",
    "check_diff(hf_rope_q, rope_q, name=\"RoPE Q\")\n",
    "check_diff(hf_rope_k, rope_k, name=\"RoPE K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "q_proj.weight torch.Size([2048, 2048])\n",
      "k_proj.weight torch.Size([512, 2048])\n",
      "v_proj.weight torch.Size([512, 2048])\n",
      "o_proj.weight torch.Size([2048, 2048])\n",
      "Attention max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal grouped-query attention.\n",
    "    GQA: https://arxiv.org/pdf/2305.13245\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, rope: RoPE):\n",
    "        super().__init__()\n",
    "        self.rope = rope\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.k_proj = nn.Linear(d_model, n_kv_heads * self.head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, n_kv_heads * self.head_size, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "hf_gqa = hf_model.model.layers[0].self_attn\n",
    "print(hf_gqa)\n",
    "my_gqa = GroupedQueryAttention(2048, 32, 8, rope).to(\"cuda\", dtype=torch.bfloat16)\n",
    "for (name, param) in hf_gqa.state_dict().items():\n",
    "    print(name, param.shape)\n",
    "    my_gqa.state_dict()[name].copy_(param)\n",
    "\n",
    "hf_attn_out = hf_gqa(rand_x, (cos, sin), None)[0]\n",
    "my_attn_out = my_gqa(rand_x)\n",
    "check_diff(hf_attn_out, my_attn_out, name=\"Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n",
      "self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "input_layernorm.weight torch.Size([2048])\n",
      "post_attention_layernorm.weight torch.Size([2048])\n",
      "Block max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, rope: RoPE) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = GroupedQueryAttention(config.d_model, config.n_heads, config.n_kv_heads, rope)\n",
    "        self.mlp = SwiGLU(config.d_model, config.d_hidden)\n",
    "        self.input_layernorm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.self_attn(self.input_layernorm(x))\n",
    "        x = x + self.mlp(self.post_attention_layernorm(x))\n",
    "        return x\n",
    "\n",
    "my_block = TransformerBlock(llama_config, rope).to(\"cuda\", dtype=torch.bfloat16)\n",
    "hf_block = hf_model.model.layers[0]\n",
    "print(hf_block)\n",
    "for (name, param) in hf_block.state_dict().items():\n",
    "    print(name, param.shape)\n",
    "    my_block.state_dict()[name].copy_(param)\n",
    "\n",
    "hf_block_out = hf_block(rand_x, position_embeddings=(cos, sin))[0]\n",
    "my_block_out = my_block(rand_x)\n",
    "check_diff(hf_block_out, my_block_out, name=\"Block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([128256, 2048])\n",
      "Logits max diff: 0.8125\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Logits not allclose",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m hf_logits \u001b[38;5;241m=\u001b[39m hf_model(tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     26\u001b[0m my_logits \u001b[38;5;241m=\u001b[39m my_llama(tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m \u001b[43mcheck_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: logits allclose fails.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mcheck_diff\u001b[0;34m(x, y, name, atol)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_diff\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, y: torch\u001b[38;5;241m.\u001b[39mTensor, name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensors\u001b[39m\u001b[38;5;124m\"\u001b[39m, atol: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m max diff: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(x\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39my)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(x, y, atol\u001b[38;5;241m=\u001b[39matol), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not allclose\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Logits not allclose"
     ]
    }
   ],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rope = RoPE(config)\n",
    "        self.model = nn.ModuleDict({\n",
    "            \"embed_tokens\": nn.Embedding(config.vocab_size, config.d_model),\n",
    "            \"layers\": nn.ModuleList([TransformerBlock(config, self.rope) for i in range(config.n_hidden_layers)]),\n",
    "            \"norm\": nn.RMSNorm(config.d_model, eps=config.rms_norm_eps),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.model.embed_tokens(x)\n",
    "        for layer in self.model.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.lm_head(self.model.norm(x))\n",
    "        return logits\n",
    "    \n",
    "my_llama = Llama(llama_config).to(\"cuda\", dtype=torch.bfloat16)\n",
    "for (name, param) in hf_model.state_dict().items():\n",
    "    print(name, param.shape)\n",
    "    my_llama.state_dict()[name].copy_(param)\n",
    "\n",
    "hf_logits = hf_model(tokens[\"input_ids\"]).logits\n",
    "my_logits = my_llama(tokens[\"input_ids\"])\n",
    "check_diff(hf_logits, my_logits[-1], name=\"Logits\")  # TODO: logits allclose fails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
