{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gordonbchen/code/ml_explore/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 1.24B\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> 128000\n",
      "<|eot_id|> 128009\n",
      "<|start_header_id|> 128006\n",
      "<|end_header_id|> 128007\n",
      "<|end_of_text|> 128001\n",
      "\n",
      "<|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|> \n",
      "\n",
      "tensor([[128000, 128006,   9125, 128007,    198,   2675,    527,    264,  11190,\n",
      "          15592,  18328,    430,    374,    459,   6335,    304,  18682,  21579,\n",
      "            323,    264,  11944,    323,   2867,  11326,     13, 128009,    198,\n",
      "         128006,    882, 128007,    198,   6854,    499,  10552,    279,    445,\n",
      "          81101,  18112,     30, 128009,    198, 128006,  78191, 128007]],\n",
      "       device='cuda:0') \n",
      "\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Llama is a large language model developed by Meta. It's a transformer-based model that's designed to perform a variety of tasks, including language translation, question answering, and text summarization. Here's a detailed explanation of how the Llama architecture works:\n",
      "\n",
      "**Overview**\n",
      "\n",
      "The Llama architecture is a type of transformer-based language model that's specifically designed for natural language processing tasks. It's composed of multiple layers, each of which is responsible for a specific task, such as tokenization, encoding, and decoding.\n",
      "\n",
      "**Layers**\n",
      "\n",
      "The Llama architecture consists of multiple layers, each of which is a sub-module within the overall model. Here's a breakdown of the different layers and their functions:\n",
      "\n",
      "1. **Encoder**: This layer is responsible for encoding the input text into a vector representation that can be processed by the model. The encoder uses a transformer architecture, which is a type of neural network that's particularly well-suited for sequence-to-sequence tasks like language translation.\n",
      "2. **Decoder**: This layer is responsible for generating the output text based\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Llama architecture is a variant of the Transformer model, a popular type of neural network used for natural language processing (NLP) tasks such as language translation, text generation, and text classification.\n",
      "\n",
      "In a traditional Transformer model, the input sequence is passed through a series of self-attention mechanisms, which allow the model to weigh the importance of each input element relative to the others. This attention mechanism is based on the dot product of the input elements, normalized by the square root of the attention mask.\n",
      "\n",
      "The Llama architecture, on the other hand, modifies this traditional Transformer model by adding a few key components:\n",
      "\n",
      "1. **Attention Mechanism**: The Llama architecture uses a modified self-attention mechanism, which is based on the Transformer model but with some key differences. The attention mechanism in Llama is called \"cross-attention\" because it allows the model to attend to different elements in the input sequence simultaneously.\n",
      "2. **Positional Encoding**: The Llama architecture adds a positional encoding layer to the input sequence, which is used to preserve the position\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "hf_llama = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "n_params = sum(p.numel() for p in hf_llama.parameters())\n",
    "print(f\"Params: {n_params / (10**9):.2f}B\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "special_tokens = {\n",
    "    \"bot\": \"<|begin_of_text|>\",\n",
    "    \"eot\": \"<|eot_id|>\",\n",
    "    \"sh\": \"<|start_header_id|>\",\n",
    "    \"eh\": \"<|end_header_id|>\",\n",
    "    \"eos\": \"<|end_of_text|>\"\n",
    "}\n",
    "for token in special_tokens.values():\n",
    "    print(token, tokenizer.convert_tokens_to_ids(token))\n",
    "print()\n",
    "\n",
    "# https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "prompt = f\"\"\"{special_tokens['sh']}system{special_tokens['eh']}\n",
    "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.{special_tokens['eot']}\n",
    "{special_tokens['sh']}user{special_tokens['eh']}\n",
    "Can you explain the Llama architecture?{special_tokens['eot']}\n",
    "{special_tokens['sh']}assistant{special_tokens['eh']}\"\"\"\n",
    "print(prompt, \"\\n\")\n",
    "\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(tokens[\"input_ids\"], \"\\n\")\n",
    "\n",
    "generated = hf_llama.generate(\n",
    "    **tokens,\n",
    "    max_length=len(tokens[\"input_ids\"]) + 256, \n",
    "    num_return_sequences=2,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=tokenizer.convert_tokens_to_ids(special_tokens[\"eot\"])\n",
    ")\n",
    "for generation in tokenizer.batch_decode(generated):\n",
    "    print(\"\\n\", \"-\"*80)\n",
    "    print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hf_llama)\n",
    "print(hf_llama.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig(vocab_size=128256, n_hidden_layers=16, d_model=2048, d_hidden=8192, rms_norm_eps=1e-05, max_pos=131072, rope_config=RopeConfig(base_freq=500000.0, scale_factor=32.0, high_freq_factor=4.0, low_freq_factor=1.0, og_max_pos=8192), d_head=64, n_heads=32, n_kv_heads=8)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RopeConfig:\n",
    "    base_freq: float = 500_000.0\n",
    "    \n",
    "    scale_factor: float = 32.0\n",
    "    high_freq_factor: float = 4.0\n",
    "    low_freq_factor: float = 1.0\n",
    "    og_max_pos: int = 8_192\n",
    "    \n",
    "@dataclass\n",
    "class LlamaConfig:\n",
    "    vocab_size: int = 128_256\n",
    "    n_hidden_layers: int = 16\n",
    "    d_model: int = 2_048\n",
    "    d_hidden: int = 8_192\n",
    "    rms_norm_eps: float = 1e-5\n",
    "\n",
    "    max_pos: int = 131_072\n",
    "    rope_config: RopeConfig = RopeConfig()\n",
    "\n",
    "    d_head: int = 64\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: int = 8\n",
    "\n",
    "llama_config = LlamaConfig()\n",
    "print(llama_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def check_diff(x: torch.Tensor, y: torch.Tensor, name: str = \"Tensors\", atol: float = 1e-5) -> None:\n",
    "    print(f\"{name} max diff: {(x - y).abs().max()}\")\n",
    "    assert torch.allclose(x, y, atol=atol), f\"{name} not allclose\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def copy_params(hf_model: nn.Module, my_model: nn.Module) -> None:\n",
    "    for name, param in hf_model.state_dict().items():\n",
    "        print(name, param.shape)\n",
    "        my_shape = my_model.state_dict()[name].shape\n",
    "        assert param.shape == my_shape, f\"Shapes don't match: hf {param.shape}, me {my_shape}\"\n",
    "        my_model.state_dict()[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_proj.weight torch.Size([8192, 2048])\n",
      "up_proj.weight torch.Size([8192, 2048])\n",
      "down_proj.weight torch.Size([2048, 8192])\n",
      "SwiGLU max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU: https://arxiv.org/pdf/2002.05202v1\n",
    "    SiLU aka Swish: https://pytorch.org/docs/stable/generated/torch.nn.functional.silu.html\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.up_proj = nn.Linear(d_model, d_hidden, bias=False)\n",
    "        self.gate_proj = nn.Linear(d_model, d_hidden, bias=False)\n",
    "        self.down_proj = nn.Linear(d_hidden, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate = F.silu(self.gate_proj(x))\n",
    "        z = self.up_proj(x)\n",
    "        return self.down_proj(gate * z)\n",
    "\n",
    "hf_swiglu = hf_llama.model.layers[0].mlp\n",
    "my_swiglu = SwiGLU(llama_config.d_model, llama_config.d_hidden).to(device=\"cuda\", dtype=torch.bfloat16)\n",
    "copy_params(hf_swiglu, my_swiglu)\n",
    "\n",
    "B, T, C = 8, 128, llama_config.d_model\n",
    "rand_x = torch.randn((B, T, C), device=\"cuda\", dtype=torch.bfloat16)\n",
    "check_diff(hf_swiglu(rand_x), my_swiglu(rand_x), name=\"SwiGLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cos max diff: 0.0\n",
      "Sin max diff: 0.0\n",
      "RoPE Q max diff: 0.0\n",
      "RoPE K max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Encoding: https://arxiv.org/pdf/2104.09864\n",
    "    HuggingFace Positional Encoding explanation: https://huggingface.co/blog/designing-positional-encoding\n",
    "    HuggingFace Llama Positional Encoding: https://github.com/huggingface/transformers/blob/41b9b92b52215bed472c9a534a06abbc3a9a95cd/src/transformers/modeling_rope_utils.py#L322\n",
    "    \"\"\"\n",
    "    def __init__(self, llama_config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        rope_config = llama_config.rope_config\n",
    "        freqs = 1.0 / rope_config.base_freq ** (torch.arange(0, llama_config.d_head, 2) / llama_config.d_head) \n",
    "        wavelens = 2 * math.pi / freqs\n",
    "\n",
    "        low_freq_wavelen = rope_config.og_max_pos / rope_config.low_freq_factor\n",
    "        freqs[wavelens > low_freq_wavelen] /= rope_config.scale_factor\n",
    "\n",
    "        smooth_factor = ((rope_config.og_max_pos / wavelens) - rope_config.low_freq_factor) / (rope_config.high_freq_factor - rope_config.low_freq_factor)\n",
    "        smoothed_freqs = ((1.0 - smooth_factor) * freqs / rope_config.scale_factor) + (smooth_factor * freqs)\n",
    "        high_freq_wavelen = rope_config.og_max_pos / rope_config.high_freq_factor\n",
    "        is_medium_freq = (wavelens <= low_freq_wavelen) & (wavelens >= high_freq_wavelen)\n",
    "        freqs = torch.where(is_medium_freq, smoothed_freqs, freqs)\n",
    "\n",
    "        pos = torch.arange(llama_config.max_pos).unsqueeze(1)\n",
    "        pthetas = pos * freqs  # (max_pos, d_head / 2)\n",
    "        self.register_buffer(\"sin\", pthetas.sin().repeat(1, 2))\n",
    "        self.register_buffer(\"cos\", pthetas.cos().repeat(1, 2))\n",
    "\n",
    "    def rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = x[..., :x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2:]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        T = x.shape[-2]\n",
    "        return (self.cos[None, None, :T] * x) + (self.sin[None, None, :T] * self.rotate_half(x))\n",
    "\n",
    "Q = torch.rand(B, llama_config.n_heads, T, llama_config.d_head, device=\"cuda\", dtype=torch.bfloat16)\n",
    "K = torch.rand_like(Q)\n",
    "rope = RoPE(llama_config).to(\"cuda\", dtype=torch.bfloat16)\n",
    "rope_q, rope_k = rope(Q), rope(K)\n",
    "\n",
    "# RoPE is a relative positional encoding, affinities on the diagonal should be the same.\n",
    "# This check does not work well for lower precision (bfloat16).\n",
    "# affin_no_rope = Q @ K.transpose(-1, -2)\n",
    "# affin_rope = rope_q @ rope_k.transpose(-1, -2)\n",
    "# affin_rope_diags = torch.diagonal(affin_rope, dim1=-2, dim2=-1)\n",
    "# affin_no_rope_diags = torch.diagonal(affin_no_rope, dim1=-2, dim2=-1)\n",
    "# check_diff(affin_rope_diags, affin_no_rope_diags, name=\"RoPE diags\")\n",
    "\n",
    "hf_rope = hf_llama.model.rotary_emb\n",
    "position_ids = torch.arange(T).expand(B, T).to(\"cuda\")\n",
    "cos, sin = hf_rope(Q, position_ids)\n",
    "check_diff(cos, rope.cos[:T], name=\"Cos\")\n",
    "check_diff(sin, rope.sin[:T], name=\"Sin\")\n",
    "hf_rope_q, hf_rope_k = apply_rotary_pos_emb(Q, K, cos, sin)\n",
    "check_diff(hf_rope_q, rope_q, name=\"RoPE Q\")\n",
    "check_diff(hf_rope_k, rope_k, name=\"RoPE K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n",
      "q_proj.weight torch.Size([2048, 2048])\n",
      "k_proj.weight torch.Size([512, 2048])\n",
      "v_proj.weight torch.Size([512, 2048])\n",
      "o_proj.weight torch.Size([2048, 2048])\n",
      "Attention max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal grouped-query attention.\n",
    "    GQA: https://arxiv.org/pdf/2305.13245\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, n_kv_heads: int, rope: RoPE):\n",
    "        super().__init__()\n",
    "        self.rope = rope\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        assert n_heads % n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_size = d_model // n_heads\n",
    "        self.k_proj = nn.Linear(d_model, n_kv_heads * self.head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, n_kv_heads * self.head_size, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        q = q.view(B, T, self.n_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_kv_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=True)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "hf_gqa = hf_llama.model.layers[0].self_attn\n",
    "print(hf_gqa)\n",
    "my_gqa = GroupedQueryAttention(2048, 32, 8, rope).to(\"cuda\", dtype=torch.bfloat16)\n",
    "copy_params(hf_gqa, my_gqa)\n",
    "\n",
    "hf_attn_out = hf_gqa(rand_x, (cos, sin), None)[0]\n",
    "my_attn_out = my_gqa(rand_x)\n",
    "check_diff(hf_attn_out, my_attn_out, name=\"Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([2048])\n",
      "RMSNorm max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, normalized_size: int, eps: float) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_size, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x_in: torch.Tensor) -> torch.Tensor:\n",
    "        x = x_in.to(torch.float32)\n",
    "        ms = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(self.eps + ms)\n",
    "        return self.weight * x.to(x_in.dtype)\n",
    "    \n",
    "hf_norm = hf_llama.model.norm\n",
    "my_norm = RMSNorm(llama_config.d_model, eps=llama_config.rms_norm_eps).to(\"cuda\", dtype=torch.bfloat16)\n",
    "copy_params(hf_norm, my_norm)\n",
    "check_diff(hf_norm(rand_x), my_norm(rand_x), \"RMSNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n",
      "self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "input_layernorm.weight torch.Size([2048])\n",
      "post_attention_layernorm.weight torch.Size([2048])\n",
      "Block max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig, rope: RoPE) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = GroupedQueryAttention(config.d_model, config.n_heads, config.n_kv_heads, rope)\n",
    "        self.mlp = SwiGLU(config.d_model, config.d_hidden)\n",
    "        self.input_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.self_attn(self.input_layernorm(x))\n",
    "        x = x + self.mlp(self.post_attention_layernorm(x))\n",
    "        return x\n",
    "\n",
    "hf_block = hf_llama.model.layers[0]\n",
    "print(hf_block)\n",
    "my_block = TransformerBlock(llama_config, rope).to(\"cuda\", dtype=torch.bfloat16)\n",
    "copy_params(hf_block, my_block)\n",
    "\n",
    "hf_block_out = hf_block(rand_x, position_embeddings=(cos, sin))[0]\n",
    "my_block_out = my_block(rand_x)\n",
    "check_diff(hf_block_out, my_block_out, name=\"Block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([8192, 2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 8192])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n",
      "lm_head.weight torch.Size([128256, 2048])\n",
      "Logits max diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "class Llama(nn.Module):\n",
    "    def __init__(self, config: LlamaConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rope = RoPE(config)\n",
    "        self.model = nn.ModuleDict({\n",
    "            \"embed_tokens\": nn.Embedding(config.vocab_size, config.d_model),\n",
    "            \"layers\": nn.ModuleList([TransformerBlock(config, self.rope) for i in range(config.n_hidden_layers)]),\n",
    "            \"norm\": RMSNorm(config.d_model, eps=config.rms_norm_eps),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.model.embed_tokens(x)\n",
    "        for layer in self.model.layers:\n",
    "            x = layer(x)\n",
    "        x = self.model.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "\n",
    "my_llama = Llama(llama_config).to(\"cuda\", dtype=torch.bfloat16)\n",
    "copy_params(hf_llama, my_llama)\n",
    "\n",
    "hf_logits = hf_llama(tokens[\"input_ids\"]).logits\n",
    "my_logits = my_llama(tokens[\"input_ids\"])\n",
    "\n",
    "# BUG: logit diff is quite high in float32, 0 in bfloat16.\n",
    "check_diff(hf_logits, my_logits, name=\"Logits\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Llama architecture! I'd be happy to explain it in detail.\n",
      "\n",
      "The Llama is a transformer-based neural network architecture developed by Meta AI, a leading artificial intelligence research organization. It's a type of large language model that's specifically designed to handle sequential data, such as text. Here's a high-level overview of how it works:\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "1. **Encoder:** The encoder is a sequence-to-sequence model that takes in a sequence of tokens (words, characters, etc.) and outputs a sequence of vectors (embeddings) that represent the input text.\n",
      "2. **Decoder:** The decoder is another sequence-to-sequence model that generates the output text based on the input sequence.\n",
      "3. **Transformer:** The transformer is a self-attention mechanism that allows the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
      "\n",
      "**How Llama Works:**\n",
      "\n",
      "Here's a step-by-step explanation of the Llama architecture:\n",
      "\n",
      "1. **Text Input:** The input text is fed into the encoder, which breaks\n",
      "\n",
      " --------------------------------------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a helpful AI assistant that is an expert in Deep Learning and a detailed and clear teacher.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Can you explain the Llama architecture?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I can explain the Llama architecture, which is a type of transformer model developed by Meta AI.\n",
      "\n",
      "**What is Llama?**\n",
      "\n",
      "The Llama (Large Language Model Meta AI) is a transformer-based language model designed to process and generate human-like language. It's an instance of the transformer architecture, which is a type of neural network that's particularly well-suited for natural language processing tasks.\n",
      "\n",
      "**Architecture Overview**\n",
      "\n",
      "The Llama architecture consists of several key components:\n",
      "\n",
      "1. **Encoder**: This is the part of the model that takes in input data (such as a sequence of words) and outputs a fixed-size representation of the input data.\n",
      "2. **Decoder**: This is the part of the model that generates output based on the input representation.\n",
      "3. **Self-Attention Mechanism**: This is the component that allows the model to attend to different parts of the input data simultaneously and weigh their importance.\n",
      "\n",
      "Here's a high-level overview of the Llama architecture:\n",
      "\n",
      "**Encoder**\n",
      "\n",
      "* Takes in a sequence of input tokens (e.g. words or\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: nn.Module,\n",
    "    tokens: torch.Tensor, \n",
    "    max_length: int, \n",
    "    n_samples: int,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 1.0\n",
    ") -> torch.tensor:\n",
    "    model.eval()\n",
    "    tokens = tokens.expand(n_samples, tokens.shape[-1])\n",
    "    while tokens.shape[-1] < max_length:\n",
    "        logits = model(tokens[:, -llama_config.max_pos:])[:, -1] / temperature\n",
    "\n",
    "        sorted_logits, sorted_inds = torch.sort(logits, dim=-1, descending=True)\n",
    "        probs = sorted_logits.softmax(dim=-1)\n",
    "        cum_probs = probs.cumsum(dim=-1)\n",
    "        \n",
    "        mask = cum_probs > top_p\n",
    "        mask[..., 1:] = mask[..., :-1]\n",
    "        mask[..., 0] = False\n",
    "\n",
    "        sorted_logits[mask] = float(\"-inf\")\n",
    "        probs = sorted_logits.softmax(dim=-1)\n",
    "\n",
    "        inds = sorted_inds.gather(-1, torch.multinomial(probs, num_samples=1))\n",
    "        tokens = torch.cat((tokens, inds), dim=-1)\n",
    "    return tokens\n",
    "\n",
    "generated = generate(my_llama, tokens[\"input_ids\"], max_length=len(tokens[\"input_ids\"]) + 256, n_samples=2, temperature=0.7, top_p=0.9)\n",
    "for generation in tokenizer.batch_decode(generated):\n",
    "    print(\"\\n\", \"-\"*80)\n",
    "    print(generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
